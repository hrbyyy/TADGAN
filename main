# coding:--utf-8--

import sys
import torch
import torch.nn as nn
from torch.nn.utils import weight_norm
import os
import argparse


import numpy as np
import pandas as pd
import torch.optim as optim
import torch.utils.data as Data
from torch.optim.lr_scheduler import MultiStepLR
import pickle
# import matplotlib.pyplot as plt
from functools import reduce
from torch.nn import BatchNorm1d
import time
import score_GAN
import gc

import math





datapath= ''


def form_dataloader(batchsize,inpath):
    trainx_new=pickle.load(open(inpath+'trainx.pkl','rb'))
    val1x_new=pickle.load(open(inpath+'val1x.pkl','rb'))
    
    testx_new = pickle.load(open(inpath + 'alltestx.pkl', 'rb'))



    train_x = torch.from_numpy(trainx_new).type(torch.FloatTensor)
    val1_x = torch.from_numpy(val1x_new).type(torch.FloatTensor)
   
    test_x = torch.from_numpy(testx_new).type(torch.FloatTensor)

    train_x=torch.transpose(train_x,1,2)
    val1_x=torch.transpose(val1_x,1,2)
  
    test_x=torch.transpose(test_x,1,2)

    torch_trainset = Data.TensorDataset(train_x, train_x)
  
    trainloader = Data.DataLoader(
        dataset=torch_trainset,  # torch TensorDataset format
        batch_size=batchsize,  # mini batch size
        shuffle=True 
    )
    val1data = Data.TensorDataset(val1_x, val1_x)
    val1loader = Data.DataLoader(dataset=val1data, shuffle=False, batch_size=batchsize)
    
    testdata = Data.TensorDataset(test_x, test_x)
    testloader = Data.DataLoader(dataset=testdata, shuffle=False, batch_size=batchsize)

    return trainloader,val1loader,testloader


class Options:
    def __init__(self,c_in,c_out,dim_z):
       
        self.c_in=c_in
        self.c_out=c_out
        self.dim_z=dim_z


class Encoder(nn.Module):
    def __init__(self,opt):
        super(Encoder, self).__init__()

    

        self.layer1=nn.LSTM(input_size=opt.c_in,hidden_size=100,batch_first=True,bidirectional=True)
        self.layer2=nn.Linear(200,opt.dim_z) 
      

    def forward(self, x):

       
        self.layer1.flatten_parameters()
        out1=self.layer1(x)

        z=self.layer2(out1[0])
        return z  



class Decoder(nn.Module):
    def __init__(self, opt):
        super(Decoder, self).__init__()


        self.layer1=nn.LSTM(input_size=opt.dim_z,num_layers=2,hidden_size=64,batch_first=True,bidirectional=True)
        self.layer2=nn.Linear(64*2,opt.c_out)
       

      
    def forward(self, z):
        self.layer1.flatten_parameters()
        out1=self.layer1(z)
        x=self.layer2(out1[0])
        return x

class Generator(nn.Module):

    def __init__(self, opt):
        super(Generator, self).__init__()
        self.encoder = Encoder(opt)
        self.decoder = Decoder(opt)

    def forward(self, x):
        latent_i = self.encoder(x)
        gen_x = self.decoder(latent_i)
        return gen_x, latent_i  


class Discriminator_z(nn.Module): 
    def __init__(self, opt):
        super(Discriminator_z, self).__init__()
       
        self.layer2=nn.Conv1d(opt.dim_z,opt.dim_z,3,1,1,bias=False)
        self.layer3=nn.Linear(opt.dim_z,1)

      
    def forward(self,x):
       
        out1=torch.transpose(x,1,2) #(b,dim_z,t)
        out2=self.layer2(out1)  #(b,dim_z,t)
        out2=torch.transpose(out2,1,2)  #(b,t,dim_z)
        out3=self.layer3(out2)  #(b,t,1)
        result=torch.squeeze(out3,dim=2)

        return result


class Discriminator_x(nn.Module):
    def __init__(self, opt):
        super(Discriminator_x, self).__init__()
      
        self.layer2 = nn.Conv1d(opt.c_out, opt.c_out, 3, 1, 1, bias=False)
        self.layer3 = nn.Linear(opt.c_out, 1)

     
    def forward(self, x):
     
        x_1= torch.transpose(x, 1, 2)  # (b,c_out,t)
        out2 = self.layer2(x_1)  # (b,c_out,t)
        out2 = torch.transpose(out2, 1, 2)  # (b,t,c_out)
        out3 = self.layer3(out2)  # (b,t,1)
        result = torch.squeeze(out3, dim=2)

        return result




def train(Generator, lambda_m,lambda_b,Discriminator_x,Discriminator_z,opt,d_clip, device, trainloader,epoch, losst_g,losst_dx,losst_dz, optimizer_g, optimizer_dx,optimizer_dz):  #lambda_m,lambda_b分别对应训练G时，MSE BCE的loss权重。
   
    rloss_dx=0
    running_loss_dx=0
    rloss_dz=0
    running_loss_dz=0
    rloss_g = 0  
    running_loss_g = 0.0  
    Generator.train()
    Discriminator_x.train()
    Discriminator_z.train()

   
    criterion_g=torch.nn.MSELoss(reduction='sum')

    for i, data in enumerate(trainloader):

        inputs = data[0][:, :, :- 1]
        targets = data[1][:, :, :-1]  
        b,t,f=inputs.shape
        inputs = inputs.to(device)
        targets = targets.to(device)
        z=torch.randn(b,t,opt.dim_z).to(device)

       
        real_label_x = torch.linspace(0.9, 1, targets.shape[0] * targets.shape[1], device=device).reshape(
            targets.shape[0], targets.shape[1])

        predict_label_x = torch.zeros(size=(targets.shape[0], targets.shape[1]), device=device)

        real_label_z = torch.linspace(0.9, 1, z.shape[0] * z.shape[1], device=device).reshape(z.shape[0], z.shape[1])

        predict_label_z = torch.zeros(size=(z.shape[0], z.shape[1]), device=device)


      
        optimizer_dx.zero_grad()

        real_out_x = Discriminator_x(targets)  

        dx_loss_real = score_GAN.binary_CE(real_out_x, real_label_x, reduction='sum') 

       
        outputs_d = Generator.module.decoder(z)

        predict_out_dx = Discriminator_x(outputs_d.detach())  
        dx_loss_fake = score_GAN.binary_CE(predict_out_dx, predict_label_x, reduction='sum')

    
        dx_loss = dx_loss_real + dx_loss_fake  
        dx_loss.backward()
      
        optimizer_dx.step()
        for para in Discriminator_x.parameters():
            if para.requires_grad_ == True:
                torch.nn.utils.clip_grad_value_(para, d_clip)

        rloss_dx += dx_loss.item()
        running_loss_dx+= dx_loss.item()


      
        optimizer_dz.zero_grad()

        real_out_z = Discriminator_z(z) 

        dz_loss_real = score_GAN.binary_CE(real_out_z, real_label_z, reduction='sum') 

     
        outputs_e=Generator.module.encoder(inputs)
     
        predict_out_dz = Discriminator_z(outputs_e.detach()) 
        dz_loss_fake = score_GAN.binary_CE(predict_out_dz, predict_label_z, reduction='sum') 

      
        dz_loss = dz_loss_real + dz_loss_fake  
        dz_loss.backward()
    
        optimizer_dz.step()
        for para in Discriminator_z.parameters():
            if para.requires_grad_ == True:
                torch.nn.utils.clip_grad_value_(para, d_clip)




        rloss_dz += dz_loss.item()
        running_loss_dz += dz_loss.item()




        optimizer_g.zero_grad()
        outputs_g,latent_g = Generator(inputs)  

     
        fake_dx_out=Discriminator_x(Generator.module.decoder(z))
        G_dx_loss=score_GAN.binary_CE(fake_dx_out,real_label_x,reduction='sum')
        fake_dz_out=Discriminator_z(latent_g)
        G_dz_loss=score_GAN.binary_CE(fake_dz_out,real_label_z,reduction='sum')
        G_d_loss=G_dx_loss+G_dz_loss
        G_rec_loss=criterion_g(outputs_g,targets)

        loss_g=lambda_m*G_rec_loss+lambda_b*G_d_loss
      
        loss_g.backward()
     
        optimizer_g.step()
       
        rloss_g += loss_g.item()

        running_loss_g += loss_g.item()


        if i % 20 == 19: 
            print('[%d, %5d] G train loss: %.3f   D train loss:%.3f'  %
                  (epoch + 1, i + 1, running_loss_g / 20,running_loss_dx/20))
            running_loss_g = 0.0
            running_loss_dx=0.0
    print('[%d] G train loss: %.3f   D train loss:%.3f' %
          (epoch + 1,  rloss_g , rloss_dx / 20))
    losst_g.append(rloss_g) 
    losst_dx.append(rloss_dx)
    losst_dz.append(rloss_dz)
    torch.cuda.empty_cache()
    del trainloader
    gc.collect()
    return losst_g,losst_dx,losst_dz









if __name__ == "__main__":

    repeats = 5
    epoches = 20
 
    d_clip = 0.01
  
    window=10
  
    dim_z=100

    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  

    batchsize = 64

    input_dim = 77


  
    output_dim = input_dim

    lambda_b = 1 
    lambda_m = 1 
    patience = 20  
    threshold = 2 

   
   

    generator = Generator(opt)
    generator = nn.DataParallel(generator)
    generator.to(device)
    discriminatorx = Discriminator_x(opt)
    discriminatorx = nn.DataParallel(discriminatorx)
    discriminatorx.to(device)
    discriminatorz = Discriminator_z(opt)
    discriminatorz = nn.DataParallel(discriminatorz)
    discriminatorz.to(device)
    optimizer_g = optim.RMSprop(generator.parameters(), lr=5 * 1e-4)
    optimizer_dz = optim.RMSprop(discriminatorz.parameters(), lr=1* 1e-4)
    optimizer_dx = optim.RMSprop(discriminatorx.parameters(), lr=1 * 1e-4)


    total_running = epoches  
    epoch_train_times = []
    for epoch in range(epoches): 
        trainloader, val1loader,  testloader = form_dataloader(batchsize, datapath)

        del testloader
        gc.collect()

        losst_g, losst_dx,losst_dz = train(generator, lambda_m, lambda_b, discriminatorx,discriminatorz,opt, d_clip, device,
                                 trainloader,  epoch, losst_g, losst_dx,losst_dz, optimizer_g,
                                 optimizer_dx,optimizer_dz)  




      
      

     
    torch.cuda.empty_cache()
    gc.collect()






